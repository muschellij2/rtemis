# s.ADDTREE.R
# ::rtemis::
# 2017 Efstathios D. Gennatas egenn.github.io
# TODO: check if factor outcome with string levels works & with addtree_path_to_rules

#' Additive Tree: Tree-Structured Boosting [C]
#'
#' Train an Additive Tree model
#'
#' For binary classification, outcome must be factor with two levels, the first level
#' is the 'positive' class
#'
#' Factor levels should not contain the "/" character (it is used to separate conditions
#' in the addtree object)
#'
#' [gS] Indicates that more than one value can be supplied, which will result in grid search using
#' internal resampling
#' lambda <- gamma/(1 - gamma)
#' @inheritParams s.GLM
#' @param x N x D matrix of N examples with D features
#' @param y N x 1 vector of labels with values in {-1,1}
#' @param catPredictors Logical vector with the same length as the feature vector, where TRUE
#'    means that the corresponding column of x is a categorical variable
#' @param gamma [gS] acceleration factor = lambda / (1 + lambda)
#' #' @param max.depth [gS] maximum depth of the tree
#' @param learning.rate [gS] learning rate for the Newton Raphson step that updates the function values
#' of the node
#' @param min.hessian [gS] Minimum second derivative to continue splitting
#' @param match.rules Logical: If TRUE, match cases to rules to get statistics per node, i.e. what
#' percent of cases match each rule. If available, these are used by [mplot3.addtree] when plotting
#' @return Object of class [rtMod]
#' @author Efstathios D. Gennatas
#' @family Supervised Learning
#' @family Tree-based methods
#' @family Interpretable models
#' @references
#' Valdes G, Luna JM, Eaton E, Simone CB, Ungar LH, Solberg TD. MediBoost: a Patient
#' Stratification Tool for Interpretable Decision Making in the Era of Precision Medicine.
#' Sci Rep. 2016;6:37854. doi:10.1038/srep37854.
#' @export

s.ADDTREE <- function(x, y = NULL,
                      x.test = NULL, y.test = NULL,
                      x.name = NULL, y.name = NULL,
                      weights = NULL,
                      update = c("exponential", "polynomial"),
                      min.update = ifelse(update == "polynomial", .035, 1000),
                      min.hessian = .001,
                      min.membership = 1,
                      steps.past.min.membership = 0,
                      gamma = .8,
                      max.depth = 30,
                      learning.rate = .1,
                      ipw = TRUE,
                      ipw.type = 2,
                      upsample = FALSE,
                      upsample.seed = NULL,
                      imetrics = TRUE,
                      grid.resample.rtset = rtset.resample("kfold", 5),
                      metric = "Balanced Accuracy",
                      maximize = TRUE,
                      prune = TRUE,
                      prune.empty.leaves = TRUE,
                      remove.bad.parents = FALSE,
                      match.rules = TRUE,
                      print.plot = TRUE,
                      plot.fitted = NULL,
                      plot.predicted = NULL,
                      plot.theme = getOption("rt.fit.theme", "lightgrid"),
                      question = NULL,
                      rtclass = NULL,
                      verbose = TRUE,
                      prune.verbose = FALSE,
                      trace = 1,
                      grid.verbose = TRUE,
                      diagnostics = FALSE,
                      outdir = NULL,
                      save.rpart = FALSE,
                      save.mod = ifelse(!is.null(outdir), TRUE, FALSE),
                      n.cores = rtCores, ...) {

  # [ INTRO ] ====
  if (missing(x)) {
    print(args(s.ADDTREE))
    return(invisible(9))
  }
  if (!is.null(outdir)) outdir <- normalizePath(outdir, mustWork = FALSE)
  logFile <- if (!is.null(outdir)) {
    paste0(outdir, "/", sys.calls()[[1]][[1]], ".", format(Sys.time(), "%Y%m%d.%H%M%S"), ".log")
  } else {
    NULL
  }
  start.time <- intro(verbose = verbose, logFile = logFile)
  mod.name <- "ADDTREE"

  # [ DEPENDENCIES ] ====
  if (!depCheck("rpart", "data.tree", verbose = FALSE)) {
    cat("\n"); stop("Please install dependencies and try again")
  }

  # [ ARGUMENTS ] ====
  if (is.null(y) & NCOL(x) < 2) {
    print(args(s.ADDTREE))
    stop("y is missing")
  }
  if (is.null(x.name)) x.name <- getName(x, "x")
  if (is.null(y.name)) y.name <- getName(y, "y")
  # prefix <- paste0(y.name, "~", x.name)
  if (!verbose) print.plot <- FALSE
  verbose <- verbose | !is.null(logFile)
  if (save.mod & is.null(outdir)) outdir <- paste0("./s.", mod.name)
  if (!is.null(outdir)) outdir <- paste0(normalizePath(outdir, mustWork = FALSE), "/")
  update <- match.arg(update)
  # if (update == "exponential") {
  #   if (!(0 <= min(gamma) & max(gamma) <= 1)) stop("gamma must be between 0 and 1")
  # }
  if (!verbose) prune.verbose <- FALSE

  # [ DATA ] ====
  dt <- dataPrepare(x, y, x.test, y.test,
                    ipw = ipw, ipw.type = ipw.type,
                    upsample = upsample, upsample.seed = upsample.seed,
                    verbose = verbose)
  x <- dt$x
  y <- dt$y
  x.test <- dt$x.test
  y.test <- dt$y.test
  xnames <- dt$xnames
  type <- dt$type
  .weights <- if (is.null(weights) & ipw) dt$weights else weights
  x0 <- if (upsample) dt$x0 else x
  y0 <- if (upsample) dt$y0 else y
  if (verbose) dataSummary(x, y, x.test, y.test, type)
  # if (verbose) parameterSummary()
  # df.train <- data.frame(y = y, x)
  if (dt$type != "Classification") stop("Only binary classification is currently supported")
  if (print.plot) {
    if (is.null(plot.fitted)) plot.fitted <- if (is.null(y.test)) TRUE else FALSE
    if (is.null(plot.predicted)) plot.predicted <- if (!is.null(y.test)) TRUE else FALSE
  } else {
    plot.fitted <- plot.predicted <- FALSE
  }

  # [ GRID SEARCH ] ====
  if (gridCheck(gamma, max.depth, learning.rate)) {
    gs <- gridSearchLearn(x0, y0,
                          mod.name,
                          resample.rtset = grid.resample.rtset,
                          grid.params = list(gamma = gamma,
                                             max.depth = max.depth,
                                             learning.rate = learning.rate,
                                             min.hessian = min.hessian),
                          fixed.params = list(catPredictors = NULL,
                                              ipw = ipw,
                                              ipw.type = ipw.type,
                                              upsample = upsample,
                                              upsample.seed = upsample.seed),
                          weights = weights,
                          metric = metric,
                          maximize = maximize,
                          verbose = verbose,
                          grid.verbose = grid.verbose,
                          n.cores = n.cores)
    gamma <- gs$best.tune$gamma
    max.depth <- gs$best.tune$max.depth
    learning.rate <- gs$best.tune$learning.rate
    min.hessian <- gs$best.tune$min.hessian
  } else {
    gs <- NULL
  }
  parameters <- list(gamma = gamma,
                     max.depth = max.depth,
                     learning.rate = learning.rate,
                     min.hessian = min.hessian,
                     ipw = ipw,
                     ipw.type = ipw.type,
                     upsample = upsample,
                     upsample.seed = upsample.seed)

  # [ ADDTREE ] ====
  if (verbose) msg("Training ADDTREE...", newline = TRUE)
  mod <- mediboost(x, y,
                   catPredictors = NULL,
                   depthLimit = max.depth,
                   learningRate = learning.rate,
                   gamma = gamma,
                   update = update,
                   min.update = min.update,
                   min.hessian = min.hessian,
                   min.membership = min.membership,
                   steps.past.min.membership = steps.past.min.membership,
                   weights = .weights,
                   save.rpart = save.rpart,
                   verbose = verbose,
                   trace = trace)

  # [ FITTED ] ====
  fitted <- predict(mod, x)
  error.train <- try(modError(y, fitted))
  if (verbose) errorSummary(error.train, mod.name)

  # [ PREDICTED ] ====
  if (!is.null(x.test)) {
    predicted <- predict(mod, x.test)
    if (!is.null(y.test)) {
      error.test <- try(modError(y.test, predicted))
      if (verbose) errorSummary(error.test, mod.name)
    } else {
      error.test <- NULL
    }
  } else {
    predicted <- error.test <- NULL
  }

  # [ OUTRO ] ====
  # rpart <- NULL
  # frame <- try(preorder.addtree(mod))
  # rpart <- list(frame = frame,
  #               method = "user")
  extra <- list(gridSearch = gs)
  rt <- rtModSet(rtclass = rtclass,
                 mod = mod,
                 mod.name = mod.name,
                 type = type,
                 parameters = parameters,
                 y.train = y,
                 y.test = y.test,
                 x.name = x.name,
                 y.name = y.name,
                 xnames = xnames,
                 fitted = fitted,
                 se.fit = NULL,
                 error.train = error.train,
                 predicted = predicted,
                 se.prediction = NULL,
                 error.test = error.test,
                 question = question,
                 extra = extra)

  # [ DATA.TREE ] ====
  if (verbose) msg("Traversing tree by preorder...")
  rt$mod$frame <- preorderTree.addtree(rt, x)
  if (verbose) msg("Converting paths to rules...")
  rt$mod$frame$Rule <- addtree_path_to_rules(rt$mod$frame$Path)
  if (verbose) msg("Converting to data.tree object...")
  rt$mod$addtree <- data.tree::as.Node(rt$mod$frame, pathName = "Path")

  # [ PRUNE ] ====
  if (verbose) msg("Pruning tree...")
  rt$mod$addtree.pruned <- prune.addtree(rt$mod$addtree,
                                         prune.empty.leaves = prune.empty.leaves,
                                         remove.bad.parents = remove.bad.parents,
                                         verbose = prune.verbose)
  if (match.rules) {
    rules <- data.tree::Get(data.tree::Traverse(rt$mod$addtree.pruned), "Rule")
    xt <- data.table::as.data.table(cbind(x, y))
    n.match <- n.pos <- vector("integer", length(rules))
    n.match[1] <- NROW(x)
    n.pos[1] <- table(y)[1]
    for (i in 2:length(rules)) {
      match <- xt[eval(parse(text = rules[i]))]
      n.match[i] <- NROW(match)
      n.pos[i] <- table(match$y)[1]
    }
    rt$extra$node.stats <- data.frame(n.match = n.match,
                                      pct.total = n.match / NROW(x),
                                      pct.pos = n.pos / n.match)
    rt$mod$addtree.pruned$Set(n.match = rt$extra$node.stats$n.match)
    rt$mod$addtree.pruned$Set(pct.total = rt$extra$node.stats$pct.total)
    rt$mod$addtree.pruned$Set(pct.pos = rt$extra$node.stats$pct.pos)
  }

  # [ iMETRICS ] ====
  rt$extra$imetrics <- list(n.nodes = rt$mod$addtree.pruned$totalCount - 1,
                            depth = rt$mod$addtree.pruned$height - 1)

  rtMod.out(rt,
            print.plot,
            plot.fitted,
            plot.predicted,
            y.test,
            mod.name,
            outdir,
            save.mod,
            verbose,
            plot.theme)

  outro(start.time, verbose = verbose, sinkOff = ifelse(is.null(logFile), FALSE, TRUE))
  rt

} # rtemis::s.ADDTREE

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rtset.R
\name{rtset}
\alias{rtset}
\alias{rtset.resample}
\alias{rtset.grid.resample}
\alias{rtset.bag.resample}
\alias{rtset.meta.resample}
\alias{rtset.cv.resample}
\alias{rtset.cluster}
\alias{rtset.color}
\alias{rtset.preprocess}
\alias{rtset.decompose}
\alias{rtset.ADDT}
\alias{rtset.GBM}
\alias{rtset.RANGER}
\alias{rtset.DN}
\alias{rtset.MXN}
\alias{rtset.lincoef}
\alias{rtset.MARS}
\title{\pkg{rtemis} default-setting functions}
\usage{
rtset.resample(resampler = "kfold", n.resamples = 10,
  stratify.var = NULL, cv.p = 0.75, cv.groups = 4,
  target.length = NULL, seed = NULL, verbose = TRUE)

rtset.grid.resample(resampler = "strat.boot", n.resamples = 10,
  stratify.var = NULL, cv.p = 0.75, cv.groups = 4,
  target.length = NULL, verbose = TRUE)

rtset.bag.resample(resampler = "strat.sub", n.resamples = 10,
  stratify.var = NULL, cv.p = 0.75, cv.groups = 4,
  target.length = NULL, verbose = TRUE)

rtset.meta.resample(resampler = "strat.sub", n.resamples = 4,
  stratify.var = NULL, cv.p = 0.75, cv.groups = 4,
  target.length = NULL, verbose = TRUE)

rtset.cv.resample(resampler = "kfold", n.resamples = 10,
  stratify.var = NULL, cv.p = 0.75, cv.groups = 4,
  target.length = NULL, verbose = TRUE)

rtset.cluster(type = "fork", hosts = NULL, n.cores = rtCores, ...)

rtset.color(n = 101, colors = NULL, space = "rgb", lo = "#01256E",
  lomid = NULL, mid = "white", midhi = NULL, hi = "#95001A",
  colorbar = FALSE, cb.mar = c(1, 1, 1, 1), ...)

rtset.preprocess(completeCases = FALSE, impute = FALSE,
  impute.type = "missForest", impute.niter = 10, impute.ntree = 500,
  impute.discrete = getMode, impute.numeric = mean,
  removeCases.thres = NULL, removeFeatures.thres = NULL,
  integer2factor = FALSE, nonzeroFactors = FALSE, scale = FALSE,
  center = FALSE, removeConstant = TRUE, oneHot = FALSE,
  verbose = TRUE)

rtset.decompose(decom = "ICA", k = 2, ...)

rtset.ADDT(max.depth = 2, learning.rate = 1, lin.type = "glmnet",
  alpha = 0, lambda = 0.1, minobsinnode = 2, minobsinnode.lin = 20,
  ...)

rtset.GBM(interaction.depth = 2, shrinkage = 0.001, max.trees = 5000,
  min.trees = 100, bag.fraction = 0.9, n.minobsinnode = 5,
  grid.resample.rtset = rtset.resample("kfold", 5), ipw = TRUE,
  upsample = FALSE, upsample.seed = NULL, ...)

rtset.RANGER(n.trees = 1000, min.node.size = 1, mtry = NULL,
  grid.resample.rtset = rtset.resample("kfold", 5), ipw = TRUE,
  upsample = FALSE, upsample.seed = NULL, ...)

rtset.DN(hidden = 1, activation = NULL, learning.rate = 0.8,
  momentum = 0.5, learningrate_scale = 1, output = NULL,
  numepochs = 100, batchsize = NULL, hidden_dropout = 0,
  visible_dropout = 0, ...)

rtset.MXN(n.hidden.nodes = NULL, output = NULL, activation = "relu",
  ctx = mxnet::mx.cpu(), optimizer = "sgd",
  initializer = mxnet::mx.init.Xavier(), batch.size = NULL,
  momentum = 0.9, max.epochs = 2000, min.epochs = 25,
  early.stop = "train", early.stop.n.steps = NULL,
  early.stop.relativeVariance.threshold = NULL, learning.rate = NULL,
  dropout = 0, dropout.before = 1, dropout.after = 0,
  eval.metric = NULL, arg.params = NULL, mx.seed = NULL)

rtset.lincoef(method = c("glmnet", "cv.glmnet", "lm.ridge", "allSubsets",
  "forwardStepwise", "backwardStepwise", "glm", "sgd", "solve"),
  alpha = 0, lambda = 0.01, lambda.seq = NULL,
  cv.glmnet.nfolds = 5, which.cv.glmnet.lambda = c("lambda.min",
  "lambda.1se"), nbest = 1, nvmax = 8, sgd.model = "glm",
  sgd.model.control = list(lambda1 = 0, lambda2 = 0),
  sgd.control = list(method = "ai-sgd"))

rtset.MARS(hidden = 1, activation = NULL, learning.rate = 0.8,
  momentum = 0.5, learningrate_scale = 1, output = NULL,
  numepochs = 100, batchsize = NULL, hidden_dropout = 0,
  visible_dropout = 0, ...)
}
\arguments{
\item{resampler}{String: Type of resampling to perform: "bootstrap", "kfold", "strat.boot", "strat.sub".
Default = "strat.boot" for \code{length(y) < 200}, otherwise "strat.sub"}

\item{n.resamples}{Integer: Number of training/testing sets required}

\item{stratify.var}{Numeric vector (optional): Variable used for stratification. Defaults to \code{y}}

\item{cv.p}{Float (0, 1): Fraction of cases to assign to traininig set for \code{resampler = "strat.sub"}}

\item{cv.groups}{Integer: Number of groups to use for stratification for
\code{resampler = "strat.sub" / "strat.boot"}}

\item{target.length}{Integer: Number of cases for training set for \code{resampler = "strat.boot"}.
Default = \code{length(y)}}

\item{seed}{Integer: (Optional) Set seed for random number generator, in order to make output reproducible.
See \code{?base::set.seed}}

\item{verbose}{Logical: If TRUE, print messages to screen}

\item{type}{String: "fork", "psock"}

\item{hosts}{Vector of strings: For type = "psock": Host names on which to run (macOS, Linux, Windows)}

\item{n.cores}{Integer: Number of cores to use on \code{localhost} for type = "fork" (macOS, Linux only)}

\item{...}{\code{rtset.cluster}: Additional argument to be passed to \code{parallel::makePSOCKcluster}}

\item{n}{Integer: How many distinct colors you want. If not odd, converted to \code{n + 1}
Defaults to 21}

\item{colors}{String: Acts as a shortcut to defining \code{lo}, \code{mid}, etc for a number of defaults:
"french", "penn", "grnblkred",}

\item{space}{String: Which colorspace to use. Option: "rgb", or "Lab". Default = "rgb".
Recommendation: If \code{mid} is "white" or "black" (default), use "rgb", otherwise "Lab"}

\item{lo}{Color for low end}

\item{lomid}{Color for low-mid}

\item{mid}{Color for middle of the range or "mean", which will result in \code{colorOp(c(lo, hi), "mean")}.
If \code{mid = NA}, then only \code{lo} and \code{hi} are used to create the color gradient.}

\item{midhi}{Color for middle-high}

\item{hi}{Color for high end}

\item{colorbar}{Logical: Create a vertical colorbar}

\item{cb.mar}{Vector, length 4: Colorbar margins. Default: c(1, 1, 1, 1)}

\item{decom}{String: Name of decomposer to use. Default = "ICA"}

\item{k}{Integer: Number of dimensions to project to. Default = 2}

\item{max.depth}{Integer: Max depth of additive tree}

\item{learning.rate}{Float: learning rate}

\item{alpha}{Float: \code{alpha} for method = \code{glmnet} or \code{cv.glmnet}. Default = 0}

\item{lambda}{Float: lambda parameter for \code{MASS::lm.ridge} Default = .01}

\item{minobsinnode}{Integer: Minimum N observations needed in node, before considering splitting}

\item{interaction.depth}{\link{gS} Integer: Interaction depth}

\item{shrinkage}{\link{gS} Float: Shrinkage (learning rate)}

\item{bag.fraction}{\link{gS} Float (0, 1): Fraction of cases to use to train each tree.
Helps avoid overfitting. Default = .75}

\item{n.minobsinnode}{\link{gS} Integer: Minimum number of observation allowed in node}

\item{grid.resample.rtset}{List: Output of \link{rtset.resample} defining \link{gridSearchLearn} parameters.
Default = \code{rtset.resample("kfold", 5)}}

\item{ipw}{Logical: If TRUE, apply inverse probability weighting (for Classification only).
Note: If \code{weights} are provided, \code{ipw} is not used. Default = TRUE}

\item{upsample}{Logical: If TRUE, upsample cases to balance outcome classes (for Classification only)
Caution: upsample will randomly sample with replacement if the length of the majority class is more than double
the length of the class you are upsampling, thereby introducing randomness}

\item{upsample.seed}{Integer: If provided, will be used to set the seed during upsampling.
Default = NULL (random seed)}

\item{n.trees}{Integer: Initial number of trees to fit}

\item{min.node.size}{\link{gS} Integer: Minimum node size}

\item{mtry}{\link{gS} Integer: Number of features sampled randomly at each split. Defaults to square root of n of
features for classification, and a third of n of features for regression.}

\item{activation}{String vector: Activation types to use: 'relu', 'sigmoid', 'softrelu', 'tanh'.
If length < n of hidden layers, elements are recycled. See \code{mxnet::mx.symbol.Activation}}

\item{output}{String: "Logistic" for binary classification, "Softmax" for classification of 2 or more classes,
"Linear" for Regression. Defaults to "Logistic" for binary outcome, "Softmax" for 3+ classes, "LinearReg" for
regression.}

\item{n.hidden.nodes}{Integer vector: Length must be equal to the number of hidden layers you wish to create}

\item{ctx}{MXNET context: \code{mxnet::mx.cpu()} to use CPU(s). Define N of cores using \code{n.cores} argument.
\code{mxnet::mx.gpu()} to use GPU. For multiple GPUs, provide list like such:
\code{ctx = list(mxnet::mx.gpu(0), mxnet::mx.gpu(1)} to use two GPUs.}

\item{max.epochs}{Integer: Number of iterations for training.}

\item{dropout}{Float (0, 1): Probability of dropping nodes}

\item{dropout.before}{Integer: Index of hidden layer before which dropout should be applied}

\item{dropout.after}{Integer: Index of hidden layer after which dropout should be applied}

\item{eval.metric}{String: Metrix used for evaluation during train. Default: "rmse"}

\item{method}{String: Method to use:
"glm": uses \code{stats::lm.wfit};
"glmnet": uses \code{glmnet::glmnet};
"cv.glmnet": uses \code{glmnet:cv.glmnet};
"lm.ridge": uses \code{MASS::lm.ridge};
"allsubsets": uses \code{leaps::regsubsets} with \code{method = "exhaustive"};
"forwardStepwise": uses \code{leaps::regsubsets} with \code{method = "forward};
"backwardStepwise": uses \code{leaps::regsubsets} with \code{method = "backward};
"sgd": uses \code{sgd::sgd}
"solve": uses \code{base::solve}}

\item{lambda.seq}{Float, vector: lambda sequence for \code{glmnet} and \code{cv.glmnet}. Default = NULL}

\item{cv.glmnet.nfolds}{Integer: Number of folds for \code{cv.glmnet}}

\item{which.cv.glmnet.lambda}{String: Whitch lambda to pick from cv.glmnet:
"lambda.min": Lambda that gives minimum cross-validated error;}

\item{nbest}{Integer: For \code{method = "allSubsets"}, number of subsets of each size to record. Default = 1}

\item{nvmax}{Integer: For \code{method = "allSubsets"}, maximum number of subsets to examine.}

\item{sgd.model}{String: Model to use for \code{method = "sgd"}. Default = "glm"}

\item{sgd.model.control}{List: \code{model.control} list to pass to \code{sgd::sgd}}

\item{sgd.control}{List: \code{sgd.control} list to pass to \code{sgd::sgd}}

\item{decom}{String: Name of decomposer to use. Default = "ICA"}

\item{k}{Integer: Number of dimensions to project to. Default = 2}
}
\value{
List with parameters
}
\description{
These functions output lists of default settings for different \pkg{rtemis} functions.
This removes the need of passing named lists of arguments, and provides autocompletion,
making it easier to setup functions without having to refer to the manual.
}
\author{
Efstathios D. Gennatas
}
